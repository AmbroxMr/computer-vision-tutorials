{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Detector & Descriptor Benchmarking\n",
    "\n",
    "Now that we have met some of the most interesting keypoint detectors and descriptors, it would be interesting to test them and compare their results in terms of number of **detections, robustness, invariance and performance**. In the context of our photo-stitching application, not all the keypoint detectors and descriptors seem to perform the same, right?.\n",
    "\n",
    "Thus, in this notebook, you are asked to **evaluate** the following methods:\n",
    "\n",
    "- Harris + NCC\n",
    "- Harris + ORB (descriptor)\n",
    "- ORB \n",
    "- SIFT\n",
    "- SURF\n",
    "\n",
    "in images that suffer **changes** in:\n",
    "\n",
    "- lighting conditions\n",
    "- rotation\n",
    "- scale\n",
    "- point of view\n",
    "\n",
    "So, for each situation, you'll be provided with a pair of images that you will have to use to **detect, describe and match** the above-mentioned keypoints. After that, plot in a bar chart the following statistics:\n",
    "\n",
    "- average number of keypoints detected in the images,\n",
    "- number of found matches,\n",
    "- time spent per keypoint at detection (including description)*, and\n",
    "- time spent per match during matching*.\n",
    "\n",
    "*use `time.process_time()` from the [`time`](https://docs.python.org/3/library/time.html) package to measure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "images_path = './images/'\n",
    "\n",
    "# for pysift\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.third_party import pysift #https://github.com/rmislam/PythonSIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output vectors\n",
    "# We have 5 methods and 4 different scenarios\n",
    "stats_kps = np.zeros((4,5))\n",
    "stats_mat = np.zeros((4,5))\n",
    "stats_tdet = np.zeros((4,5))\n",
    "stats_tmat = np.zeros((4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 Preliminary functions\n",
    "\n",
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1a:  For Harris and NCC (brought from 4.1 and adapted)</i></b></span>**\n",
    "First, take the code you implemented in notebook 4.1 for the Harris method and convert it in two functions to:\n",
    "- detect Harris keypoints (return a list of `cv2.Keypoint`).\n",
    "- match Harris keypoints using NCC (return a list of `cv2.DMatch`). Use in here the `non-max-suppresion` method we provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect Harris\n",
    "def detectHarris(image,w_size,sobel_size,k):\n",
    "    # compute Harris\n",
    "    harris = cv2.cornerHarris(image,w_size,sobel_size,k)\n",
    "    # perform non-max-suppression\n",
    "    r,c = nonmaxsuppts(harris, 3, 0.1*harris.max())\n",
    "    # convert to a keypoint list\n",
    "    kps = [cv2.KeyPoint(c[i], r[i], 1) for i in range(len(r))]\n",
    "    \n",
    "    return kps\n",
    "    \n",
    "# ... and another one to match them\n",
    "def matchHarris(image_l,image_r,kps_l,kps_r):\n",
    "    # augment border\n",
    "    w_temp = 20 # window size\n",
    "    image_l_pad = cv2.copyMakeBorder(image_l, w_temp, w_temp, w_temp, w_temp, cv2.BORDER_REFLECT)\n",
    "    image_r_pad = cv2.copyMakeBorder(image_r, w_temp, w_temp, w_temp, w_temp, cv2.BORDER_REFLECT)\n",
    "    \n",
    "    # initialize matches list\n",
    "    matches = []\n",
    "    \n",
    "    c_r = [int(round(kps_r[i].pt[0])) for i in range(len(kps_r))]\n",
    "    r_r = [int(round(kps_r[i].pt[1])) for i in range(len(kps_r))]\n",
    "    \n",
    "    # for each keypoint in left image\n",
    "    for p_index in range(len(kps_l)):\n",
    "\n",
    "        c_l = int(round(kps_l[p_index].pt[0]))\n",
    "        r_l = int(round(kps_l[p_index].pt[1]))\n",
    "        \n",
    "        # get the template (note the w_temp offset because of the padding in previous step)\n",
    "        p_r, p_c = r_l+w_temp, c_l+w_temp\n",
    "        template = image_l_pad[p_r-w_temp:p_r+w_temp+1,p_c-w_temp:p_c+w_temp+1]\n",
    "\n",
    "        # compute NCC of the left keypoint in right image\n",
    "        ncc = cv2.matchTemplate(image_r_pad, template, cv2.TM_CCORR_NORMED)\n",
    "\n",
    "        # find max value in NCC (only at the right keypoints)\n",
    "        max_value = np.amax(ncc[r_r,c_r])\n",
    "\n",
    "        # if match is good\n",
    "        if max_value > 0.95:\n",
    "\n",
    "            # include in match list\n",
    "            [max_index] = np.where(ncc[r_r,c_r] == max_value)\n",
    "            matches.append((p_index, max_index))\n",
    "\n",
    "    # cast matches list to cv2.DMatch list\n",
    "    matches = np.asarray(matches)\n",
    "    matches = [cv2.DMatch(matches[i,0], matches[i,1],1) for i in range(matches.shape[0])]\n",
    "        \n",
    "    return matches\n",
    "\n",
    "# This method has been provided to you\n",
    "from scipy import signal\n",
    "def nonmaxsuppts(cim, radius, thresh):\n",
    "    \"\"\" Binarize and apply non-maximum suppresion.   \n",
    "    \n",
    "        Args:\n",
    "            cim: the harris 'R' image\n",
    "            radius: the aperture size of local maxima window\n",
    "            thresh: the threshold value for binarization\n",
    "                    \n",
    "        Returns: \n",
    "            r, c: two numpy vectors being the row (r) and the column (c) of each keypoint\n",
    "    \"\"\"   \n",
    "    \n",
    "    rows, cols = np.shape(cim)\n",
    "    sze = 2 * radius + 1\n",
    "    mx = signal.order_filter(cim, np.ones([sze, sze]), sze ** 2 - 1)\n",
    "    bordermask = np.zeros([rows, cols]);\n",
    "    bordermask[radius:(rows - radius), radius:(cols - radius)] = 1\n",
    "    cim = np.array(cim)\n",
    "    r, c = np.where((cim == mx) & (cim > thresh) & (bordermask == 1))\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1b:  Create a process function for each method</i></b></span>** \n",
    "\n",
    "Now, **create a method** for each of the proposed algorithms:\n",
    "- Harris + NCC\n",
    "- Harris + ORB descriptor\n",
    "- ORB\n",
    "- SIFT\n",
    "- SURF\n",
    "\n",
    "that performs all the desired tasks for a pair of input images (insert both the grayscale and the color version of them). These functions should do the following:\n",
    "- compute keypoints and descriptors from the grayscale image (also store the number of detected keypoint and measure the time spent in the process)\n",
    "- find matches (also store the number of matches found and measure the time spent in the process)\n",
    "- plot the resulting matches on the color images.\n",
    "- return the **average number of keypoints per image**, the **number of matches**, the **detection time** and the **matching time**.\n",
    "\n",
    "**Once you have this, you just need to call them for each pair of images!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Harris_NCC(image_l, image_l_gray, image_r, image_r_gray):\n",
    "    # parameters:\n",
    "    w_size = 2\n",
    "    sobel_size = 3\n",
    "    k = 0.05\n",
    "\n",
    "    # compute Harris corners\n",
    "    st_det = time.process_time() # Start timer\n",
    "    kps_l = detectHarris(image_l_gray,w_size,sobel_size,k)\n",
    "    kps_r = detectHarris(image_r_gray,w_size,sobel_size,k) # same for the right ones\n",
    "    det_time = round(time.process_time()-st_det,5)\n",
    "\n",
    "    kps = len(kps_l)+len(kps_r)\n",
    "\n",
    "    # save partial stats\n",
    "    num_kps = kps*0.5 # average number of keypoints\n",
    "    tdet = det_time/kps\n",
    "    \n",
    "    # match using NCC\n",
    "    st_mat = time.process_time() # Start timer\n",
    "    matches = matchHarris(image_l_gray, image_r_gray, kps_l, kps_r)\n",
    "    mat_time = round(time.process_time()-st_mat,5)\n",
    "\n",
    "    # save partial stats\n",
    "    num_matches = len(matches)\n",
    "    tmat = mat_time/num_matches\n",
    "    \n",
    "    # plot results\n",
    "    this_image = np.copy(image_l)\n",
    "    this_image = cv2.drawMatches(image_l,kps_l,image_r,kps_r,matches,this_image,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(this_image)\n",
    "    \n",
    "    return num_kps, num_matches, tdet, tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Harris_ORB(image_l, image_l_gray, image_r, image_r_gray):\n",
    "    # parameters:\n",
    "    w_size = 2\n",
    "    sobel_size = 3\n",
    "    k = 0.05\n",
    "\n",
    "    # compute Harris corners\n",
    "    st_det = time.process_time() # Start timer\n",
    "    kps_l = detectHarris(image_l_gray,w_size,sobel_size,k)\n",
    "    kps_r = detectHarris(image_r_gray,w_size,sobel_size,k) # same for the right ones\n",
    "    det_time = round(time.process_time()-st_det,5)\n",
    "\n",
    "    kps = len(kps_l)+len(kps_r)\n",
    "\n",
    "    # save partial stats\n",
    "    num_kps = kps*0.5 # average number of keypoints\n",
    "    tdet = det_time/kps\n",
    "    \n",
    "    # compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    st_det_desc = time.process_time() # Start timer\n",
    "    kps_l, des_l = orb.compute(image_l_gray, kps_l)\n",
    "    kps_r, des_r = orb.compute(image_r_gray, kps_r)\n",
    "    det_desc_time = round(time.process_time()-st_det_desc,5)\n",
    "\n",
    "    # update partial stats\n",
    "    tdet = tdet + det_desc_time/num_kps # add descriptor computation to detection time\n",
    "    \n",
    "    # Match descriptors.\n",
    "    st_mat = time.process_time() # Start timer\n",
    "    matches = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True).match(des_l,des_r)\n",
    "    mat_time = round(time.process_time()-st_mat,5)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_matches = len(matches)\n",
    "    tmat = mat_time/num_matches\n",
    "\n",
    "    # plot results\n",
    "    this_image = np.copy(image_l)\n",
    "    this_image = cv2.drawMatches(image_l,kps_l,image_r,kps_r,matches,this_image,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(this_image)\n",
    "    \n",
    "    return num_kps, num_matches, tdet, tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ORB(image_l, image_l_gray, image_r, image_r_gray):\n",
    "    # -- create the ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # -- detect ORB keypoints\n",
    "    st_det = time.process_time() # Start timer\n",
    "    kps_l = orb.detect(image_l_gray,None)\n",
    "    kps_r = orb.detect(image_r_gray,None)\n",
    "\n",
    "    # -- compute the descriptors with ORB\n",
    "    kps_l, des_l = orb.compute(image_l_gray, kps_l)\n",
    "    kps_r, des_r = orb.compute(image_r_gray, kps_r)\n",
    "    det_time = round(time.process_time()-st_det,5)\n",
    "    \n",
    "    kps = len(kps_l)+len(kps_r)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_kps = kps*0.5 # average number of keypoints\n",
    "    tdet = det_time/kps\n",
    "\n",
    "    # Match descriptors.\n",
    "    st_mat = time.process_time() # Start timer\n",
    "    matches = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True).match(des_l,des_r)\n",
    "    mat_time = round(time.process_time()-st_mat,5)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_matches = len(matches)\n",
    "    tmat = mat_time/num_matches\n",
    "\n",
    "    # Display both images side-by-side along with the matches\n",
    "    this_image = np.copy(image_l)\n",
    "    this_image = cv2.drawMatches(image_l,kps_l,image_r,kps_r,matches,this_image,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(this_image)\n",
    "    \n",
    "    return num_kps, num_matches, tdet, tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_SIFT(image_l, image_l_gray, image_r, image_r_gray):\n",
    "    \n",
    "    # OpenCV\n",
    "#     sift = cv2.xfeatures2d.SIFT_create(300)\n",
    "#     st_det = time.process_time() # Start timer\n",
    "#     kps_l, des_l = sift.detectAndCompute(image_l_gray,None)\n",
    "#     kps_r, des_r = sift.detectAndCompute(image_r_gray,None)\n",
    "#     det_time = round(time.process_time()-st_det,5)\n",
    "    \n",
    "    # pysift\n",
    "    st_det = time.process_time() # Start timer\n",
    "    kps_l, des_l = pysift.computeKeypointsAndDescriptors(image_l_gray)\n",
    "    kps_r, des_r = pysift.computeKeypointsAndDescriptors(image_r_gray)\n",
    "    det_time = round(time.process_time()-st_det,5)\n",
    "    \n",
    "    kps = len(kps_l)+len(kps_r)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_kps = kps*0.5 # average number of keypoints\n",
    "    tdet = det_time/kps\n",
    "\n",
    "    # Call knnMatch\n",
    "    st_mat = time.process_time() # Start timer\n",
    "    matches = cv2.BFMatcher().knnMatch(des_l,des_r, k=2)\n",
    "    good = []\n",
    "    # For each match\n",
    "    for m,n in matches:\n",
    "        # If first two distances are not close\n",
    "        if m.distance < 0.75*n.distance:\n",
    "            # It is a good match! Add it to the list\n",
    "            good.append(m)\n",
    "    mat_time = round(time.process_time()-st_mat,5)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_matches = len(good)\n",
    "    tmat = mat_time/num_matches\n",
    "\n",
    "    # Display both images side-by-side along with the matches\n",
    "    this_image = np.copy(image_l)\n",
    "    this_image = cv2.drawMatches(image_l,kps_l,image_r,kps_r,good,this_image, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(this_image)\n",
    "    \n",
    "    return num_kps, num_matches, tdet, tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_SURF(image_l, image_l_gray, image_r, image_r_gray):\n",
    "    \n",
    "    surf = cv2.xfeatures2d.SURF_create(2000)\n",
    "    \n",
    "    st_det = time.process_time() # Start timer\n",
    "    kps_l, des_l = surf.detectAndCompute(image_l_gray,None)\n",
    "    kps_r, des_r = surf.detectAndCompute(image_r_gray,None)\n",
    "    det_time = round(time.process_time()-st_det,5)\n",
    "    \n",
    "    kps = len(kps_l)+len(kps_r)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_kps = kps*0.5 # average number of keypoints\n",
    "    tdet = det_time/kps\n",
    "    \n",
    "    # Call knnMatch\n",
    "    st_mat = time.process_time() # Start timer\n",
    "    matches = cv2.BFMatcher().knnMatch(des_l,des_r, k=2)\n",
    "    good = []\n",
    "    # For each match\n",
    "    for m,n in matches:\n",
    "        # If first two distances are not close\n",
    "        if m.distance < 0.60*n.distance:\n",
    "            # It is a good match! Add it to the list\n",
    "            good.append(m)\n",
    "    mat_time = round(time.process_time()-st_mat,5)\n",
    "    \n",
    "    # save partial stats\n",
    "    num_matches = len(good)\n",
    "    tmat = mat_time/num_matches\n",
    "\n",
    "    # Display both images side-by-side along with the matches\n",
    "    this_image = np.copy(image_l)\n",
    "    this_image = cv2.drawMatches(image_l,kps_l,image_r,kps_r,good,this_image, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(this_image)\n",
    "    \n",
    "    return num_kps, num_matches, tdet, tmat    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Testing the methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2a: Changes in lighting conditions</i></b></span>** \n",
    "\n",
    "Use `bright1.png` and `bright2.png` images, which contain the same scene but with different lighting conditions.\n",
    "\n",
    "<img src=\"./images/bright1.png\" width=\"300\" align=\"left\"/><img src=\"./images/bright2.png\" width=\"300\" align=\"rigth\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the images and convert them to gray (they will be used for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "## Read images and convert to gray\n",
    "image_l = cv2.imread(images_path + 'bright1.png')\n",
    "image_r = cv2.imread(images_path + 'bright2.png')\n",
    "image_l = cv2.cvtColor(image_l,cv2.COLOR_BGR2RGB)\n",
    "image_r = cv2.cvtColor(image_r,cv2.COLOR_BGR2RGB)\n",
    "image_l_gray = cv2.cvtColor(image_l,cv2.COLOR_RGB2GRAY)\n",
    "image_r_gray = cv2.cvtColor(image_r,cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + NCC\n",
    "stats_kps[0,0],stats_mat[0,0],stats_tdet[0,0],stats_tmat[0,0] = process_Harris_NCC(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + ORB\n",
    "stats_kps[0,1],stats_mat[0,1],stats_tdet[0,1],stats_tmat[0,1] = process_Harris_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB\n",
    "stats_kps[0,2],stats_mat[0,2],stats_tdet[0,2],stats_tmat[0,2] = process_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "stats_kps[0,3],stats_mat[0,3],stats_tdet[0,3],stats_tmat[0,3] = process_SIFT(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SURF\n",
    "stats_kps[0,4],stats_mat[0,4],stats_tdet[0,4],stats_tmat[0,4] = process_SURF(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2b: Changes in rotation</i></b></span>** \n",
    "\n",
    "Use `rotate1.png` and `rotate2.png` images.\n",
    "\n",
    "<img src=\"./images/rotate1.png\" width=\"300\" align=\"left\"/><img src=\"./images/rotate2.png\" width=\"300\" align=\"rigth\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the images and convert them to gray (they will be used for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "## Read images and convert to gray\n",
    "image_l = cv2.imread(images_path + 'rotate1.png')\n",
    "image_r = cv2.imread(images_path + 'rotate2.png')\n",
    "image_l = cv2.cvtColor(image_l,cv2.COLOR_BGR2RGB)\n",
    "image_r = cv2.cvtColor(image_r,cv2.COLOR_BGR2RGB)\n",
    "image_l_gray = cv2.cvtColor(image_l,cv2.COLOR_RGB2GRAY)\n",
    "image_r_gray = cv2.cvtColor(image_r,cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + NCC\n",
    "stats_kps[1,0],stats_mat[1,0],stats_tdet[1,0],stats_tmat[1,0] = process_Harris_NCC(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + ORB\n",
    "stats_kps[1,1],stats_mat[1,1],stats_tdet[1,1],stats_tmat[1,1] = process_Harris_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB\n",
    "stats_kps[1,2],stats_mat[1,2],stats_tdet[1,2],stats_tmat[1,2] = process_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "stats_kps[1,3],stats_mat[1,3],stats_tdet[1,3],stats_tmat[1,3] = process_SIFT(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SURF\n",
    "stats_kps[1,4],stats_mat[1,4],stats_tdet[1,4],stats_tmat[1,4] = process_SURF(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2c: Changes in scale</i></b></span>** \n",
    "\n",
    "Use `scale1.png` and `scale2.png` images.\n",
    "\n",
    "<img src=\"./images/scale1.png\" width=\"300\" align=\"left\"/><img src=\"./images/scale2.png\" width=\"300\" align=\"rigth\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the images and convert them to gray (they will be used for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "## Read images and convert to gray\n",
    "image_l = cv2.imread(images_path + 'scale1.png')\n",
    "image_r = cv2.imread(images_path + 'scale2.png')\n",
    "image_l = cv2.cvtColor(image_l,cv2.COLOR_BGR2RGB)\n",
    "image_r = cv2.cvtColor(image_r,cv2.COLOR_BGR2RGB)\n",
    "image_l_gray = cv2.cvtColor(image_l,cv2.COLOR_RGB2GRAY)\n",
    "image_r_gray = cv2.cvtColor(image_r,cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + NCC\n",
    "stats_kps[2,0],stats_mat[2,0],stats_tdet[2,0],stats_tmat[2,0] = process_Harris_NCC(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + ORB\n",
    "stats_kps[2,1],stats_mat[2,1],stats_tdet[2,1],stats_tmat[2,1] = process_Harris_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB\n",
    "stats_kps[2,2],stats_mat[2,2],stats_tdet[2,2],stats_tmat[2,2] = process_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "stats_kps[2,3],stats_mat[2,3],stats_tdet[2,3],stats_tmat[2,3] = process_SIFT(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SURF\n",
    "stats_kps[2,4],stats_mat[2,4],stats_tdet[2,4],stats_tmat[2,4] = process_SURF(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2d: Changes in point of view</i></b></span>** \n",
    "\n",
    "Use `pov1.png` and `pov2.png` images.\n",
    "\n",
    "<img src=\"./images/pov1.png\" width=\"300\" align=\"left\"/><img src=\"./images/pov2.png\" width=\"300\" align=\"rigth\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the images and convert them to gray (they will be used for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "## Read images and convert to gray\n",
    "image_l = cv2.imread(images_path + 'pov1.png')\n",
    "image_r = cv2.imread(images_path + 'pov2.png')\n",
    "image_l = cv2.cvtColor(image_l,cv2.COLOR_BGR2RGB)\n",
    "image_r = cv2.cvtColor(image_r,cv2.COLOR_BGR2RGB)\n",
    "image_l_gray = cv2.cvtColor(image_l,cv2.COLOR_RGB2GRAY)\n",
    "image_r_gray = cv2.cvtColor(image_r,cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + NCC\n",
    "stats_kps[3,0],stats_mat[3,0],stats_tdet[3,0],stats_tmat[3,0] = process_Harris_NCC(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARRIS + ORB\n",
    "stats_kps[3,1],stats_mat[3,1],stats_tdet[3,1],stats_tmat[3,1] = process_Harris_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB\n",
    "stats_kps[3,2],stats_mat[3,2],stats_tdet[3,2],stats_tmat[3,2] = process_ORB(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "stats_kps[3,3],stats_mat[3,3],stats_tdet[3,3],stats_tmat[3,3] = process_SIFT(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SURF\n",
    "stats_kps[3,4],stats_mat[3,4],stats_tdet[3,4],stats_tmat[3,4] = process_SURF(image_l, image_l_gray, image_r, image_r_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.3 Visually analyzing the results\n",
    "\n",
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Plot the results</i></b></span>** \n",
    "\n",
    "Now generate plots with the following metrics:\n",
    "- average number of keypoints detected in the images\n",
    "- number of found matches\n",
    "- time spent per keypoint at detection (including description)\n",
    "- time spent per match during matching\n",
    "\n",
    "Create a 4x4 plot with [bar](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.bar.html) graphs with a **row for each metric** (*#keypoints*, *#matches*, *detection time*, and *matching time*) and a **column for each test** (*lighting conditions*, *rotation*, *scale* and *point-of-view*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphs\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "\n",
    "titles = ('Lighting', 'Rotation', 'Scale', 'Point-of-view')\n",
    "objects = ('H+NCC', 'H+ORB', 'ORB', 'SIFT', 'SURF')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "# keypoints\n",
    "for i in range(1,5):\n",
    "    plt.subplot(4,4,i)\n",
    "    plt.bar(y_pos, stats_kps[i-1,:], align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('# kps')\n",
    "    plt.title(titles[i-1])\n",
    "\n",
    "# matches\n",
    "for i in range(1,5):\n",
    "    plt.subplot(4,4,i+4)\n",
    "    plt.bar(y_pos, stats_mat[i-1,:], align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('# matches')\n",
    "    plt.title(titles[i-1])\n",
    "\n",
    "# time per detection\n",
    "for i in range(1,5):\n",
    "    ax = plt.subplot(4,4,i+8)\n",
    "    ax.set_yscale('log')\n",
    "    plt.bar(y_pos, stats_tdet[i-1,:], align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('t_det [s]')\n",
    "    plt.title(titles[i-1])\n",
    "\n",
    "# time per match\n",
    "for i in range(1,5):\n",
    "    ax = plt.subplot(4,4,i+12)\n",
    "    ax.set_yscale('log')\n",
    "    plt.bar(y_pos, stats_tmat[i-1,:], align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('t_mat [s]')\n",
    "    plt.title(titles[i-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now you have finished these tests, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are the evaluated methods invariant to these changes? <br />\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Which one would you use if you should work with each kind of images? <br />\n",
    "    \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "  \n",
    "- Which one would you use if you need a real-time system? <br />\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "  \n",
    "- If there is any method NOT invariant against a certain change, can you think in any solution to make it more robust against that? <br />\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "591.85px",
    "left": "878.583px",
    "right": "20px",
    "top": "120px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
